{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Giraldo0712/TAM_2025_2/blob/main/Parcial_1_TAM/Puntos_2_y_3_Parcial_TAM_2025_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Punto 2**"
      ],
      "metadata": {
        "id": "U2szDQdeI1Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cuadro comparativo de modelos de regresi√≥n (Scikit-learn)\n",
        "\n",
        "A continuaci√≥n se presenta una comparaci√≥n de diferentes modelos de regresi√≥n implementados en *Scikit-learn*.  \n",
        "En cada caso se analiza el modelo matem√°tico, la funci√≥n de costo que minimizan, el m√©todo de optimizaci√≥n que utilizan, su relaci√≥n con la regresi√≥n lineal b√°sica y su escalabilidad.\n",
        "\n",
        "| Modelo | Modelo matem√°tico | Funci√≥n de costo | Estrategia de optimizaci√≥n | Relaci√≥n con modelos b√°sicos | Escalabilidad |\n",
        "|---------|------------------|------------------|-----------------------------|-------------------------------|----------------|\n",
        "| **LinearRegression** | $$y = Xw + b$$ | Minimiza el **error cuadr√°tico medio (MSE)**: $$J(w) = \\frac{1}{2N}\\|y - Xw\\|^2$$ | Se obtiene de forma **anal√≠tica** resolviendo las ecuaciones normales o mediante descomposici√≥n SVD. | Es el modelo base de la regresi√≥n lineal (m√≠nimos cuadrados ordinarios). Sirve como punto de partida para los modelos con regularizaci√≥n. | Funciona muy bien con conjuntos de datos peque√±os o medianos. Su costo crece con el cubo del n√∫mero de variables (\\(O(n^3)\\)). |\n",
        "| **Lasso** | $$y = Xw + b$$ | Minimiza el MSE con una **penalizaci√≥n L1**: $$J(w) = \\frac{1}{2N}\\|y - Xw\\|^2 + \\lambda \\|w\\|_1$$ | Usa **coordinate descent**, ajustando cada peso de forma iterativa. | Se basa en la regresi√≥n lineal, pero agrega una penalizaci√≥n que obliga a que algunos pesos sean exactamente cero (selecci√≥n de variables). | Escalable para un n√∫mero moderado de variables, aunque se vuelve m√°s costoso con muchas caracter√≠sticas. |\n",
        "| **ElasticNet** | $$y = Xw + b$$ | Combina las penalizaciones L1 y L2: $$J(w) = \\frac{1}{2N}\\|y - Xw\\|^2 + \\alpha [\\rho\\|w\\|_1 + (1-\\rho)\\|w\\|_2^2]$$ | Usa **coordinate descent**, igual que Lasso, pero combinando ambos tipos de regularizaci√≥n. | Es un punto medio entre Lasso y Ridge; mantiene estabilidad sin perder capacidad de selecci√≥n de variables. | M√°s estable que Lasso cuando hay variables correlacionadas. Escalable para datasets medianos. |\n",
        "| **KernelRidge** | $$y = K\\alpha$$, donde \\(K\\) es la matriz de kernel | Minimiza $$J(\\alpha) = \\|y - K\\alpha\\|^2 + \\lambda \\|\\alpha\\|^2$$ | Tiene una **soluci√≥n cerrada** similar a Ridge, pero en el espacio de caracter√≠sticas definido por el kernel. | Extiende la regresi√≥n Ridge a relaciones no lineales gracias al uso de kernels. | Escalabilidad limitada, ya que necesita invertir una matriz de tama√±o \\(n \\times n\\). Complejidad \\(O(n^3)\\). |\n",
        "| **SGDRegressor** | $$y = Xw + b$$ | Minimiza una funci√≥n de p√©rdida (por ejemplo, MSE) m√°s regularizaci√≥n L1/L2. | Utiliza **descenso estoc√°stico del gradiente (SGD)**, actualizando los pesos con peque√±os lotes de datos. | Es una versi√≥n iterativa de modelos como Ridge o Lasso, √∫til cuando el dataset es muy grande. | Muy escalable y r√°pido; se adapta bien a big data o flujos de datos continuos. |\n",
        "| **BayesianRidge** | $$y = Xw + b + \\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$$ | Maximiza la **verosimilitud bayesiana** suponiendo priors Gaussianos sobre \\(w\\). | Calcula una **distribuci√≥n posterior anal√≠tica** para \\(w\\) y \\(\\sigma^2\\). | Se puede ver como una versi√≥n probabil√≠stica de Ridge; en vez de un punto √≥ptimo, produce una distribuci√≥n sobre los coeficientes. | Escalable para conjuntos medianos; m√°s costoso que Ridge por el c√°lculo de la inferencia bayesiana. |\n",
        "| **GaussianProcessRegressor** | $$f(x) \\sim \\mathcal{GP}(m(x), k(x,x'))$$ | Maximiza la **log-verosimilitud marginal** del proceso Gaussiano. | Ajusta los **hiperpar√°metros del kernel** mediante gradiente. | Es un modelo completamente bayesiano y no param√©trico; generaliza KernelRidge en un marco probabil√≠stico. | Poco escalable: necesita invertir una matriz \\(n \\times n\\) (\\(O(n^3)\\)). Ideal para datasets peque√±os. |\n",
        "| **SVR (Support Vector Regressor)** | $$f(x) = w^\\top \\phi(x) + b$$ | Minimiza una p√©rdida **Œµ-insensible** con penalizaci√≥n L2. | Usa **programaci√≥n cuadr√°tica convexa** para encontrar los vectores soporte. | Ampl√≠a la regresi√≥n lineal usando kernels y un margen de tolerancia Œµ alrededor de los datos. | Escalabilidad limitada (entre \\(O(n^2)\\) y \\(O(n^3)\\)), aunque eficaz para datasets medianos. |\n",
        "| **RandomForestRegressor** | $$f(x) = \\frac{1}{M}\\sum_{m=1}^M f_m(x)$$ | MSE promediado entre √°rboles. | Entrena muchos √°rboles de decisi√≥n usando **bagging** y selecci√≥n aleatoria de caracter√≠sticas. | No deriva de la regresi√≥n lineal; aprende relaciones no lineales de forma autom√°tica. | Muy escalable y paralelizable; se adapta bien a grandes vol√∫menes de datos. |\n",
        "| **GradientBoosting / XGBoost** | $$f(x) = \\sum_m \\gamma_m h_m(x)$$ | Minimiza una p√©rdida diferenciable (MSE, MAE, etc.) agregando √°rboles secuencialmente. | Usa **gradiente descendente** sobre √°rboles base. | Es un ensamble aditivo que mejora el error de forma iterativa. XGBoost es una versi√≥n optimizada y m√°s r√°pida. | Extremadamente escalable, con soporte para CPU y GPU, ideal para competiciones y grandes datasets. |\n"
      ],
      "metadata": {
        "id": "GDtmrsfsI5_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Punto 3**"
      ],
      "metadata": {
        "id": "QIOiZcflLtWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßÆ Punto 3 ‚Äî Librer√≠a RAPIDS y comparaci√≥n de regresores\n",
        "\n",
        "## ¬øQu√© es RAPIDS?\n",
        "\n",
        "**RAPIDS** es un ecosistema de librer√≠as de *machine learning* y *data science* desarrollado por **NVIDIA**, dise√±ado para ejecutar todo el flujo de trabajo de an√°lisis de datos directamente en la **GPU**.\n",
        "\n",
        "RAPIDS est√° construido sobre la arquitectura **CUDA** y permite realizar operaciones que normalmente se ejecutan en CPU (como en *pandas* o *scikit-learn*) de forma **masivamente paralela** en la GPU, logrando **aceleraciones de hasta 100 veces** en ciertos casos.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Componentes principales\n",
        "\n",
        "| Componente | Funci√≥n principal | Equivalente en CPU |\n",
        "|-------------|-------------------|--------------------|\n",
        "| **cuDF** | Manipulaci√≥n de datos tabulares en GPU | `pandas` |\n",
        "| **cuML** | Algoritmos de *machine learning* cl√°sicos (regresi√≥n, clasificaci√≥n, clustering, PCA, etc.) | `scikit-learn` |\n",
        "| **cuGraph** | An√°lisis de grafos | `NetworkX` |\n",
        "| **cuSignal / cuSpatial / cuXfilter** | Se√±ales, datos geoespaciales y visualizaci√≥n | M√≥dulos especializados |\n",
        "\n",
        "En este punto nos centraremos en **cuML**, ya que implementa los modelos de regresi√≥n equivalentes a los vistos en *Scikit-learn*.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Funcionamiento general de RAPIDS\n",
        "\n",
        "1. **Carga de datos:** Se utilizan estructuras `cuDF.DataFrame`, an√°logas a los `DataFrame` de *pandas*, pero almacenadas directamente en memoria GPU.\n",
        "2. **Entrenamiento:** Los modelos de `cuML` se ejecutan completamente en la GPU, sin transferencias de datos hacia la CPU.\n",
        "3. **Predicci√≥n:** Tambi√©n se realiza en GPU, reduciendo los cuellos de botella de comunicaci√≥n.\n",
        "\n",
        "Este flujo hace que RAPIDS sea especialmente √∫til en entornos de **Big Data** o cuando se requiere **entrenar modelos de manera iterativa** y r√°pida.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Comparaci√≥n entre modelos de *Scikit-learn* y *RAPIDS (cuML)*\n",
        "\n",
        "| **Modelo (Scikit-learn)** | **Implementaci√≥n equivalente en RAPIDS (cuML)** | **Principales hiperpar√°metros** | **Estrategia de optimizaci√≥n** | **Comentarios / Diferencias** |\n",
        "|-----------------------------|-------------------------------------------------|----------------------------------|--------------------------------|--------------------------------|\n",
        "| **LinearRegression** | `cuml.LinearRegression` | `fit_intercept`, `normalize`, `algorithm` | Soluci√≥n anal√≠tica (ecuaciones normales o QR) | Misma API que sklearn; c√°lculo en GPU. |\n",
        "| **Ridge (M√≠nimos cuadrados regularizados)** | `cuml.Ridge` | `alpha`, `fit_intercept`, `solver` | Soluci√≥n cerrada (anal√≠tica) | Igual a Ridge cl√°sico pero acelerado en GPU. |\n",
        "| **Lasso** | `cuml.Lasso` | `alpha`, `max_iter`, `tol` | Coordinate Descent | Id√©ntico a sklearn, pero m√°s r√°pido para datos grandes. |\n",
        "| **ElasticNet** | `cuml.ElasticNet` | `alpha`, `l1_ratio`, `max_iter`, `tol` | Coordinate Descent | Combina regularizaci√≥n L1 y L2 igual que en sklearn. |\n",
        "| **KernelRidge** | ‚Äî *(no implementado directamente)* | ‚Äî | ‚Äî | Puede aproximarse con `cuml.SVR` usando kernel RBF o lineal. |\n",
        "| **SGDRegressor** | `cuml.SGD` *(clasificador o regresor)* | `loss`, `penalty`, `alpha`, `learning_rate`, `eta0` | Descenso estoc√°stico del gradiente | Muy escalable y eficiente en GPU. |\n",
        "| **BayesianRidge** | ‚Äî *(no disponible en cuML)* | ‚Äî | ‚Äî | Puede aproximarse con modelos lineales o PyMC3 en GPU. |\n",
        "| **GaussianProcessRegressor** | ‚Äî *(no disponible a√∫n en cuML)* | ‚Äî | ‚Äî | Computacionalmente costoso; no escalable a GPU actualmente. |\n",
        "| **SVR (Support Vector Regressor)** | `cuml.SVR` | `C`, `epsilon`, `kernel`, `degree`, `gamma`, `coef0` | Programaci√≥n cuadr√°tica convexa | Soporta kernels RBF, lineal y polinomial. |\n",
        "| **RandomForestRegressor** | `cuml.ensemble.RandomForestRegressor` | `n_estimators`, `max_depth`, `max_features`, `n_bins` | Bagging de √°rboles GPU-paralelo | Entrenamiento masivo en paralelo, resultados id√©nticos a sklearn. |\n",
        "| **GradientBoosting / XGBoost** | `cuml.ensemble.GradientBoostingRegressor` o `xgboost.XGBRegressor(tree_method='gpu_hist')` | `n_estimators`, `learning_rate`, `max_depth`, `subsample` | Boosting por gradiente | Totalmente optimizado para GPU; escalabilidad extrema. |\n"
      ],
      "metadata": {
        "id": "JsKtUzAkLvX2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nk5i1_UP-fFO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}